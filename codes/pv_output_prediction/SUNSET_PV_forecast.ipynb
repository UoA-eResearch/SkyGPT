{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import h5py\n",
    "import matplotlib.dates as mdates\n",
    "import numpy.ma as ma\n",
    "import CRPS.CRPS as pscore\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tensorflow version\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "# check available gpu\n",
    "gpus =  tf.config.list_physical_devices('GPU')\n",
    "print(\"available gpus:\", gpus)\n",
    "# limit the gpu usage, prevent it from allocating all gpu memory for a simple model\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# check number of cpus available\n",
    "print(\"available cpus:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data location and load data\n",
    "cwd = os.getcwd()\n",
    "pardir = os.path.dirname(os.path.dirname(cwd))\n",
    "data_folder = os.path.join(pardir,\"data\")\n",
    "data_path = os.path.join(data_folder,'video_prediction_dataset.hdf5')\n",
    "\n",
    "# !change model name for different models!\n",
    "model_name = 'SUNSET_PV_forecast'\n",
    "output_folder = os.path.join(cwd,\"model_output\", model_name)\n",
    "if os.path.isdir(output_folder)==False:\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "print(\"data_folder:\", data_folder)\n",
    "print(\"data_path:\", data_path)\n",
    "print(\"output_folder:\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate handler for the hdf5 data\n",
    "forecast_dataset = h5py.File(data_path, 'r')\n",
    "\n",
    "# show structure of the hdf5 data\n",
    "def get_all(name):\n",
    "    if name!=None:\n",
    "        print(forecast_dataset[name])\n",
    "    \n",
    "forecast_dataset.visit(get_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*50)\n",
    "# get the input dimension for constructing the model\n",
    "# the input images will be reshaped from (None, 8, 64, 64, 3) to (None, 64, 64, 24)\n",
    "img_side_len = forecast_dataset['trainval']['images_log'].shape[2]\n",
    "num_log_term = forecast_dataset['trainval']['images_log'].shape[1]\n",
    "num_pred_term = forecast_dataset['trainval']['pv_pred'].shape[1]\n",
    "num_color_channel = forecast_dataset['trainval']['images_log'].shape[4]\n",
    "image_input_dim = [img_side_len,img_side_len,num_log_term*num_color_channel]\n",
    "\n",
    "print(\"image side length:\", img_side_len)\n",
    "print(\"number of log terms:\", num_log_term)\n",
    "print(\"number of pred terms:\", num_pred_term)\n",
    "print(\"number of color channels:\", num_color_channel)\n",
    "print(\"input image dimension:\", image_input_dim)\n",
    "\n",
    "# load time stamps into the memory\n",
    "times_trainval = np.load(os.path.join(data_folder,\"dataset_all\",\"times_curr_trainval.npy\"),allow_pickle=True)\n",
    "print(\"times_trainval.shape:\", times_trainval.shape)\n",
    "\n",
    "# read through the dataset once in order to cache it but not store it into the memory\n",
    "## read the data by batch\n",
    "batch_size = 10000\n",
    "num_samples = len(times_trainval)\n",
    "indices = np.arange(num_samples)\n",
    "print('-'*50)\n",
    "print('data reading start...')\n",
    "for i in range(int(num_samples / batch_size) + 1):\n",
    "    start_time = time.time()\n",
    "    start_idx = (i * batch_size) % num_samples\n",
    "    idxs = indices[start_idx:start_idx + batch_size]\n",
    "    _ = forecast_dataset['trainval']['images_log'][idxs]\n",
    "    _ = forecast_dataset['trainval']['pv_log'][idxs]\n",
    "    _ = forecast_dataset['trainval']['pv_pred'][idxs]\n",
    "    end_time = time.time()\n",
    "    print(\"batch {0} samples: {1} to {2}, {3:.2f}% finished, processing time {4:.2f}s\"\n",
    "          .format(i+1, idxs[0],idxs[-1],(idxs[-1]/num_samples)*100,(end_time-start_time)))\n",
    "\n",
    "# temporially close the dataset, will use \"with\" statement to open it when we use it\n",
    "forecast_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data pipeline helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# day block shuffling of the time stamps, and return shuffled indices\n",
    "def day_block_shuffle(times_trainval):\n",
    "    \n",
    "    # Only keep the date of each time point\n",
    "    dates_trainval = np.zeros_like(times_trainval, dtype=datetime.date)\n",
    "    for i in range(len(times_trainval)):\n",
    "        dates_trainval[i] = times_trainval[i].date()\n",
    "\n",
    "    # Chop the indices into blocks, so that each block contains the indices of the same day\n",
    "    unique_dates = np.unique(dates_trainval)\n",
    "    blocks = []\n",
    "    for i in range(len(unique_dates)):\n",
    "        blocks.append(np.where(dates_trainval == unique_dates[i])[0])\n",
    "\n",
    "    # shuffle the blocks, and chain it back together\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(blocks)\n",
    "    shuffled_indices = np.asarray(list(itertools.chain.from_iterable(blocks)))\n",
    "\n",
    "    return shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a cross validation generator function for spliting the dayblock shuffled indices into training and validation\n",
    "def cv_split(split_data, fold_index, num_fold):\n",
    "    '''\n",
    "    input:\n",
    "    split_data: the dayblock shuffled indices to be splitted\n",
    "    fold_index: the ith fold chosen as the validation, used for generating the seed for random shuffling\n",
    "    num_fold: N-fold cross validation\n",
    "    output:\n",
    "    data_train: the train data indices\n",
    "    data_val: the validation data indices\n",
    "    '''\n",
    "    # randomly divides into a training set and a validation set\n",
    "    num_samples = len(split_data)\n",
    "    indices = np.arange(num_samples)\n",
    "\n",
    "    # finding training and validation indices\n",
    "    val_mask = np.zeros(len(indices), dtype=bool)\n",
    "    val_mask[int(fold_index / num_fold * num_samples):int((fold_index + 1) / num_fold * num_samples)] = True\n",
    "    val_indices = indices[val_mask]\n",
    "    train_indices = indices[np.logical_not(val_mask)]\n",
    "\n",
    "    # shuffle indices\n",
    "    np.random.seed(fold_index)\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "    \n",
    "    data_train = split_data[train_indices]\n",
    "    data_val = split_data[val_indices]\n",
    "\n",
    "    return data_train,data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image_data):\n",
    "    '''\n",
    "    image data processing: reshaping and normalization\n",
    "    '''\n",
    "    ## reshape the image tensor from [None,8,64,64,3] to [None,64,64,24]\n",
    "    image_data = tf.transpose(image_data,perm=[0,2,3,1,4])\n",
    "    image_data = tf.reshape(image_data, [image_data.shape[0],image_data.shape[1],image_data.shape[2],-1])\n",
    "\n",
    "    ## normalize the image to [0,1]\n",
    "    image_data = tf.image.convert_image_dtype(image_data, tf.float32)\n",
    "\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a mapping function from the indices to the corresponding features and labels \n",
    "def data_loader(hdf5_data_path,sample_idx,batch_size=256):\n",
    "    '''\n",
    "    input:\n",
    "    hdf5_data_path: path to hdf5 data file\n",
    "    sample_idx: \n",
    "        for training and validation:\n",
    "            dayblock shuffled indices with cross-validation split into training and validation\n",
    "            either training or validation indices will be input\n",
    "        for testing: the indices are not shuffled\n",
    "    is_trainval: a flag, True for trainig and validation\n",
    "    output:\n",
    "    dataset: dataset for training, validation\n",
    "    '''\n",
    "\n",
    "    def mapping_func_py(hdf5_data_path,sample_idx):\n",
    "        '''\n",
    "        mapping indices to corresponding images and pviance data in hdf5 (python expression)\n",
    "        '''\n",
    "        # convert EagerTensor to str or numpy array\n",
    "        hdf5_data_path = hdf5_data_path.numpy().decode() \n",
    "        # sort the sample indices as hdf5 requires increasing order index for data retrieval\n",
    "        sample_idx = sorted(sample_idx.numpy())\n",
    "\n",
    "        with h5py.File(hdf5_data_path,'r') as f:\n",
    "\n",
    "            # read in the data\n",
    "            images_log = f['trainval']['images_log'][sample_idx]\n",
    "            pv_log = f['trainval']['pv_log'][sample_idx]\n",
    "            pv_pred = f['trainval']['pv_pred'][sample_idx][:,-1]\n",
    "\n",
    "            # process image data\n",
    "            images_log = process_image(images_log)\n",
    "            \n",
    "            # convert pv data to tf.tensor\n",
    "            pv_log = tf.convert_to_tensor(pv_log, dtype=tf.float32)\n",
    "            pv_pred = tf.convert_to_tensor(pv_pred, dtype=tf.float32)\n",
    "\n",
    "            return images_log, pv_log, pv_pred\n",
    "\n",
    "    def mapping_func_tf(hdf5_data_path,sample_idx):\n",
    "        '''\n",
    "        a wrapper mapping function to get the nested data structure \n",
    "        the output type of tf.py_function cannot be a nested sequence when using a tf.py_function with the tf.data API\n",
    "        '''\n",
    "        images_log, pv_log, pv_pred = tf.py_function(func=mapping_func_py,\n",
    "                                                           inp=[hdf5_data_path, sample_idx], \n",
    "                                                           Tout=(tf.float32, tf.float32, tf.float32))\n",
    "        return (images_log, pv_log), pv_pred\n",
    "    \n",
    "    \n",
    "    # create the indices dataset\n",
    "    idx_ds = tf.data.Dataset.from_tensor_slices(sample_idx)\n",
    "    # shuffle and batch the indices\n",
    "    idx_ds = idx_ds.shuffle(buffer_size = idx_ds.cardinality().numpy(),seed=0)\n",
    "    idx_ds = idx_ds.batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # indices dataset mapping to images and pviance data\n",
    "    # returning dataset with the following nested structure: (images_log, pv_log), pv_pred\n",
    "    dataset = idx_ds.map(lambda x: mapping_func_tf(hdf5_data_path,x),\n",
    "                         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_winkler_score(prob_prediction,observation):\n",
    "    alpha = 0.1\n",
    "    lb = np.percentile(prob_prediction,5,axis=0)\n",
    "    ub = np.percentile(prob_prediction,95,axis=0)\n",
    "    delta = ub-lb\n",
    "    if observation<lb:\n",
    "        sc = delta+2*(lb-observation)/alpha\n",
    "    if observation>ub:\n",
    "        sc = delta+2*(observation-ub)/alpha\n",
    "    if (observation>=lb) and (observation<=ub):\n",
    "        sc = delta\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model characteristics\n",
    "num_filters = 24\n",
    "kernel_size = [3,3]\n",
    "pool_size = [2,2]\n",
    "strides = 2\n",
    "dense_size = 1024\n",
    "drop_rate = 0.4\n",
    "\n",
    "# define training time parameters\n",
    "num_epochs = 200 #(The maximum epoches set to 200 and there might be early stopping depends on validation loss)\n",
    "num_fold = 10 # 10-fold cross-validation\n",
    "batch_size = 256\n",
    "learning_rate = 3e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model architecture using tf.keras API\n",
    "def sunset_model():\n",
    "    ## input\n",
    "    ### input image logs with shape (64,64,24)\n",
    "    x_in = keras.Input(shape=image_input_dim)\n",
    "    ### input pviance/pv output logs with shape (8)\n",
    "    x2_in = keras.Input(shape=num_log_term)\n",
    "\n",
    "    ## 1st convolution block\n",
    "    x = keras.layers.Conv2D(num_filters,kernel_size,padding=\"same\",activation='relu')(x_in)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n",
    "\n",
    "    ## 2nd convolution block\n",
    "    x = keras.layers.Conv2D(num_filters*2,kernel_size,padding=\"same\",activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n",
    "\n",
    "    ## 3rd convolution block\n",
    "    #x = keras.layers.Conv2D(num_filters*4,kernel_size,padding=\"same\",activation='relu')(x)\n",
    "    #x = keras.layers.BatchNormalization()(x)\n",
    "    #x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n",
    "\n",
    "    ## two fully connected nets\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Concatenate(axis=1)([x, x2_in])\n",
    "\n",
    "    x = keras.layers.Dense(dense_size, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(drop_rate)(x)\n",
    "    x = keras.layers.Dense(dense_size, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(drop_rate)(x)\n",
    "\n",
    "    ## regression to prediction target\n",
    "    y_out = keras.layers.Dense(units=1)(x)\n",
    "\n",
    "    # construct the model\n",
    "    model = keras.Model(inputs=[x_in, x2_in],outputs=y_out)\n",
    "\n",
    "    return model\n",
    "\n",
    "# show model architecture\n",
    "sunset_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate dayblock shuffled indices\n",
    "indices_dayblock_shuffled = day_block_shuffle(times_trainval)\n",
    "\n",
    "# initialize loss history list\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "    \n",
    "for i in range(num_fold):\n",
    "    \n",
    "    # construct and compile model for each repetition to reinitialize the model weights\n",
    "    keras.backend.clear_session()\n",
    "    model = sunset_model()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate),loss='mse')\n",
    "    \n",
    "    # implementing 10-fold cross-validation\n",
    "    print('Repetition {0} model training started ...'.format(i+1))\n",
    "    \n",
    "    # creating folder for saving model checkpoint\n",
    "    save_directory = os.path.join(output_folder,'repetition_'+str(i+1))\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "        \n",
    "    # training and validation data preparation\n",
    "    ## generate indices dataset for training and validation\n",
    "    indices_train, indices_val = cv_split(indices_dayblock_shuffled,i,num_fold)\n",
    "    ## load data from dataloader\n",
    "    ds_train_batched = data_loader(data_path,indices_train)\n",
    "    ds_val_batched = data_loader(data_path,indices_val,batch_size=500)\n",
    "\n",
    "    # define callbacks for training\n",
    "    ## early stopping rule: if the validation loss stop decreasing for 5 consecutive epoches\n",
    "    earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    ## model check point: save model checkpoint for later use\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(save_directory,'best_model_repitition_'+str(i+1)+'.h5'), \n",
    "                                monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "    # training the model and record training and validation loss\n",
    "    history = model.fit(ds_train_batched, epochs=num_epochs, steps_per_epoch=len(indices_train)//batch_size+1,\n",
    "                               verbose=1, callbacks=[earlystop,checkpoint], validation_data=ds_val_batched,\n",
    "                              validation_steps=len(indices_val)//batch_size+1)\n",
    "    train_loss_hist.append(history.history['loss'])\n",
    "    val_loss_hist.append(history.history['val_loss'])\n",
    "\n",
    "    # plot training and validation history\n",
    "    plt.plot(train_loss_hist[i],label='train')\n",
    "    plt.plot(val_loss_hist[i],label='validation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# saving model training and validation loss history\n",
    "np.save(os.path.join(output_folder,'train_loss_hist.npy'),train_loss_hist)\n",
    "np.save(os.path.join(output_folder,'val_loss_hist.npy'),val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of training and validation results\n",
    "best_train_loss_MSE = np.zeros(num_fold)\n",
    "best_val_loss_MSE = np.zeros(num_fold)\n",
    "\n",
    "for i in range(num_fold):\n",
    "    best_val_loss_MSE[i] = np.min(val_loss_hist[i])\n",
    "    idx = np.argmin(val_loss_hist[i])\n",
    "    best_train_loss_MSE[i] = train_loss_hist[i][idx]\n",
    "    print('Model {0}  -- train loss: {1:.2f}, validation loss: {2:.2f} (RMSE)'.format(i+1, np.sqrt(best_train_loss_MSE[i]), np.sqrt(best_val_loss_MSE[i])))\n",
    "print('The mean train loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_train_loss_MSE))))\n",
    "print('The mean validation loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_val_loss_MSE))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing data\n",
    "times_test = np.load(os.path.join(data_folder,\"dataset_all\",\"times_curr_test.npy\"),allow_pickle=True)\n",
    "print(\"times_test.shape:\", times_test.shape)\n",
    "\n",
    "with h5py.File(data_path,'r') as f:\n",
    "\n",
    "    # read in the data\n",
    "    images_log_test = f['test']['images_log'][...]\n",
    "    pv_log_test = f['test']['pv_log'][...]\n",
    "    pv_pred_test = f['test']['pv_pred'][...][:,-1]\n",
    "\n",
    "    # process image data\n",
    "    images_log_test = process_image(images_log_test).numpy()\n",
    "\n",
    "print(\"images_log_test.shape:\",images_log_test.shape)\n",
    "print(\"pv_log_test.shape:\",pv_log_test.shape)\n",
    "print(\"pv_pred_test.shape:\",pv_pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate model on the test set and generate predictions\n",
    "loss = np.zeros((num_fold,len(times_test)))\n",
    "prediction = np.zeros((num_fold,len(times_test)))\n",
    "\n",
    "for i in range(num_fold):\n",
    "    # define model path\n",
    "    print(\"loading repetition {0} model ...\".format(i+1))\n",
    "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
    "    # load the trained model\n",
    "    model = keras.models.load_model(model_path)\n",
    "    \n",
    "    # model evaluation\n",
    "    print(\"evaluating performance for the model\".format(i+1))\n",
    "    loss[i] = model.evaluate(x=[images_log_test,pv_log_test], y=pv_pred_test, batch_size=200, verbose=1)\n",
    "    \n",
    "    # generate prediction\n",
    "    print(\"generating predictions for the model\".format(i+1))\n",
    "    prediction[i] = np.squeeze(model.predict([images_log_test,pv_log_test], batch_size=200, verbose=1))\n",
    "\n",
    "# saving predictions from each model\n",
    "np.save(os.path.join(output_folder,'test_predictions.npy'),prediction)\n",
    "\n",
    "# using the ensemble mean of the 10 models as the final prediction \n",
    "print('-'*50)\n",
    "print(\"model ensembling ...\")\n",
    "prediction_ensemble = np.mean(prediction,axis=0)\n",
    "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_pred_test)**2))\n",
    "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing data\n",
    "times_test = np.load(os.path.join(data_folder,\"dataset_all\",\"times_curr_test.npy\"),allow_pickle=True)\n",
    "print(\"times_test.shape:\", times_test.shape)\n",
    "\n",
    "with h5py.File(data_path,'r') as f:\n",
    "\n",
    "    # read in the data\n",
    "    pv_pred_test = f['test']['pv_pred'][...][:,-1]\n",
    "\n",
    "print(\"pv_pred_test.shape:\",pv_pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.load(os.path.join(output_folder,'test_predictions.npy'))\n",
    "\n",
    "# using the ensemble mean of the 10 models as the final prediction \n",
    "prediction_ensemble = np.mean(prediction,axis=0)\n",
    "\n",
    "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_pred_test)**2))\n",
    "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))\n",
    "\n",
    "MAE = np.mean(np.abs((prediction_ensemble-pv_pred_test)))\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## probabilistic eval metrics\n",
    "# CRPS evaluation\n",
    "crps = np.zeros(len(times_test))\n",
    "for i in range(len(times_test)):\n",
    "    crps[i],_,_ = pscore(prediction[:,i],pv_pred_test[i]).compute()\n",
    "crps_mean = np.mean(crps)\n",
    "print('The mean crps: {0:.3f}'.format(crps_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## probabilistic eval metrics\n",
    "# Winkler score evaluation\n",
    "wscore = np.zeros(len(times_test))\n",
    "for i in range(len(times_test)):\n",
    "    wscore[i] = compute_winkler_score(prediction[:,i],pv_pred_test[i])\n",
    "wscore_mean = np.mean(wscore)\n",
    "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formulate sunny and cloudy test days\n",
    "sunny_dates = [(2017,5,20),(2017,10,7),(2017,11,1),\n",
    "        (2018,5,28),(2018,6,29),(2018,10,13),\n",
    "        (2019,6,23),(2019,7,31),(2019,8,13),(2019,10,21)]\n",
    "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
    "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
    "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
    "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
    "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
    "\n",
    "dates_test = np.asarray([times.date() for times in times_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
    "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
    "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
    "percent95_prediction = np.percentile(prediction,95,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualization of forecast prediction_ensembles\n",
    "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
    "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
    "\n",
    "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
    "xfmt = mdates.DateFormatter('%H')\n",
    "fmt_date = datetime.date(2000,1,1)\n",
    "\n",
    "green = '#8AB8A7'\n",
    "red = '#8C1515'\n",
    "blue = '#67AFD2'\n",
    "grey =  '#B6B1A9'\n",
    "black = '#2E2D29'\n",
    "red = '#8C1515'\n",
    "light_blue = '#67AFD2'\n",
    "dark_blue = '#016895'\n",
    "blue = '#4298B5'\n",
    "black = '#2E2D29'\n",
    "dark_red = '#820000'\n",
    "light_red = '#B83A4B'\n",
    "\n",
    "for i,date in enumerate(cloudy_dates_test):\n",
    "    ax = axarr[i]\n",
    "    date_mask = (dates_test == date)\n",
    "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]] \n",
    "    \n",
    "    rmse = np.sqrt(np.mean(np.square((pv_pred_test[date_mask]-prediction_ensemble[date_mask]))))\n",
    "    mae = np.mean(np.abs((pv_pred_test[date_mask]-prediction_ensemble[date_mask])))\n",
    "    \n",
    "    ax.plot(hours_xaxis, pv_pred_test[date_mask], linewidth = 1, color=black, label = 'Ground truth')\n",
    "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
    "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
    "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1, label = 'SUNSET forecast',color=red,markerfacecolor=\"None\")\n",
    "    ax.set_ylabel('PV output (kW)')\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
    "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
    "\n",
    "axarr[0].set_ylim(0, 30)\n",
    "axarr[0].legend(bbox_to_anchor= [.5,1.2], loc = 'upper center', ncol = 4)\n",
    "axarr[-1].set_xlabel('Hour of day')\n",
    "\n",
    "f.set_size_inches(15,30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a932b1910cda6042a88474b97c74999e68c6f95060e8a502ffc7e3dcbf4c7b99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
